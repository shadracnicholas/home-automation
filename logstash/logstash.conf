# logstash.conf
#
# This configuration accepts syslogs from remote servers and Docker logs that have been harvested by Filebeat.
# A best effort attempt is made to parse syslogs and to parse messages as slog lines. If successful, the parsed message
# overwrites the message field. If the data is available, the following fields will also be set in the output:
#     - service: The name of the home automation service e.g. service.config
#     - @timestamp: The time in the slog line if available, else the time in the syslog if available, else the time the log was received by Logstash.
#     - raw_message: The original message before parsing
#     - metadata: A JSON object of extra fields pulled from the slog line.
# Logs are written to stdout and to /var/log/messages

input {
    # Accept syslog messages over TCP
    tcp {
        port => 7514 # Ports in the range 1-1024 are privileged
        type => syslog # This is used below to process syslog messages
    }

    # Accept Filebeat messages
    beats {
        port => 5044
        # Filebeat spews a load of errors when its connection to Logstash is terminated
        # so increase this to 5 minutes instead of the default 60 seconds
        client_inactivity_timeout => 300
    }
}

filter {

  # Make a copy of the original message before we overwrite it
  mutate {
    copy => { "[message]" => "[raw_message]" }
  }

  if [type] == "syslog" {
    # Try to match syslog messages. There are multiple syslog standards so this might work universally.
    grok {
      match     => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:message}" }
      overwrite => [ "message" ]
    }

    # The program part of the syslog line should be prefixed with "ha-" (this is how rsyslog on the Raspberry Pis knows which
    # messages to forward to logstash in the first place). If the prefix exists, the rest of the program should be the service name.
    grok {
        match => {
          "syslog_program" => [
            "(?:(ha-))%{GREEDYDATA:service}",
            "%{DATA:syslog_program}" # Match literally anything so this filter can't fail
          ]
        }
    }

    # We will later try to replace @timestamp with a slog timestamp, but set it now in case this turns out not to be a slog line.
    date {
      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }
  }

  if [input][type] == "docker" {
    # Assume the service name is the docker-compose service name
    mutate {
      copy => { "[docker][container][labels][com_docker_compose_service]" => "[service]" }
    }
  }

  # For all messages, regardless of source, try to parse them as a known log format.
  grok {
    match => {
      # The message field will have already had other data (e.g. syslog stuff) stripped
      "message" => [
        "%{TIMESTAMP_ISO8601:log_timestamp} %{LOGLEVEL:severity} %{DATA:message} (?<metadata>({.*}))", # Match with metadata
        "%{TIMESTAMP_ISO8601:log_timestamp}(?:\s*)%{LOGLEVEL:severity}(?:\s*)%{GREEDYDATA:message}", # Match without metadata (and arbitrary whitespace)
        "%{DATA:message}" # Match literally anything so this filter can't fail
      ]
    }
    overwrite => [ "message" ]
  }

  # If we have a log_timestamp, use that as @timestamp. If @timestamp was already set above, this will overwrite it.
  if [log_timestamp] {
    date {
        match => [ "log_timestamp", "ISO8601" ] # Log lines must have an ISO8601-formatted date
    }
  }

  # Add a unique ID to each log line
  uuid {
    target    => "uuid"
    overwrite => true
  }
}

# This must be a separate filter so that we can use the skip_on_invalid_json option. This lets
# us try to parse slog_metadata without worrying about it not being there or being invalid JSON.
filter {
    json {
      source               => "metadata"
      target               => "metadata" # Put the parsed version back in the same field
      skip_on_invalid_json => true
    }
}

output {
    stdout { codec => rubydebug } # Useful for debugging
    file {
        # The path uses the joda time format and the Logstash sprintf formatting directive as documented
        # here: https://www.elastic.co/guide/en/logstash/current/event-dependent-configuration.html#sprintf
        path => "/var/log/messages-%{+YYYY-MM-dd}"
    }
}
